<!doctype html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  
    class="html theme--light"
  
><head>
  <meta charset="utf-8" />
  <title>
    
      
        Planar Data Classification with One Hidden Layer |
      Swapnil Revankar

  </title>

  <meta name="generator" content="Hugo 0.140.2"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="Swapnil Revankar" />
  <meta
    name="description"
    content="Bridging Art and Code"
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.8d4fad7e6916ad2e291e8d97ada157c70350d6d7150fea137e7243340967befb.css"
      integrity="sha256-jU&#43;tfmkWrS4pHo2XraFXxwNQ1tcVD&#43;oTfnJDNAlnvvs="
      crossorigin="anonymous"
      type="text/css"
    />
  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css"
    integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.10cb17328b3207590ce3d89fb482e7cd6937d8138cef2059c69cabd65ab7d6c6.css"
    integrity="sha256-EMsXMosyB1kM49iftILnzWk32BOM7yBZxpyr1lq31sY="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.7aa4d559d5fadcfed666b5679be54d0dbbcf6a0542742319765b253d2797cc44.css"
    integrity="sha256-eqTVWdX63P7WZrVnm&#43;VNDbvPagVCdCMZdlslPSeXzEQ="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.a699c3ab02229020e2d51dfe6c9a4cc556fd7781edbc166e2af9d0db9d5c66ce.css"
    integrity="sha256-ppnDqwIikCDi1R3&#43;bJpMxVb9d4HtvBZuKvnQ251cZs4="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.1d88b60c86c4bb253fa4d4c085d27f7ee1f6eca544f7b50b227a2e63fbcfbaaa.css"
    integrity="sha256-HYi2DIbEuyU/pNTAhdJ/fuH27KVE97ULInouY/vPuqo="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicons/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png" />

  <link rel="canonical" href="https://thetechartguy.com/post/deep-learning-week3/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js"
      integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc="
      crossorigin="anonymous"
    ></script>
  

  

  


  
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://thetechartguy.com/images/site-feature-image.png">
  <meta name="twitter:title" content="Planar Data Classification with One Hidden Layer">
  <meta name="twitter:description" content="For Week 3 of the Deep Learning Specialization, we move beyond logistic regression and build our first neural network — one with a single hidden layer. The task is to classify a toy flower-shaped dataset, which logistic regression cannot handle well.">



  
  <meta property="og:url" content="https://thetechartguy.com/post/deep-learning-week3/">
  <meta property="og:site_name" content="My blog">
  <meta property="og:title" content="Planar Data Classification with One Hidden Layer">
  <meta property="og:description" content="For Week 3 of the Deep Learning Specialization, we move beyond logistic regression and build our first neural network — one with a single hidden layer. The task is to classify a toy flower-shaped dataset, which logistic regression cannot handle well.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-09-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-09-15T00:00:00+00:00">
    <meta property="article:tag" content="Coursera">
    <meta property="article:tag" content="Deeplearning">
    <meta property="article:tag" content="Neuralnetworks">
    <meta property="og:image" content="https://thetechartguy.com/images/site-feature-image.png">
      <meta property="og:see_also" content="https://thetechartguy.com/post/deep-learning-week1-and-week2/">



  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "post",
        "name": "Planar Data Classification with One Hidden Layer",
        "headline": "Planar Data Classification with One Hidden Layer",
        "alternativeHeadline": "",
        "description": "
      
        \u003cp\u003eFor Week 3 of the Deep Learning Specialization, we move beyond logistic regression and build our \u003cstrong\u003efirst neural network\u003c\/strong\u003e — one with a single hidden layer. The task is to classify a toy \u003cstrong\u003eflower-shaped dataset\u003c\/strong\u003e, which logistic regression cannot handle well.\u003c\/p\u003e


      


    ",
        "inLanguage": "en",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/thetechartguy.com\/post\/deep-learning-week3\/"
        },
        "author" : {
            "@type": "Person",
            "name": "Swapnil Revankar"
        },
        "creator" : {
            "@type": "Person",
            "name": "Swapnil Revankar"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "Swapnil Revankar"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "Swapnil Revankar"
        },
        "copyrightYear" : "2025",
        "dateCreated": "2025-09-15T00:00:00.00Z",
        "datePublished": "2025-09-15T00:00:00.00Z",
        "dateModified": "2025-09-15T00:00:00.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "Swapnil Revankar",
            "url": "https://thetechartguy.com/",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/thetechartguy.com\/favicons\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
        
        "https://thetechartguy.com/images/site-feature-image.png"


      
      ]

    ,
        "url" : "https:\/\/thetechartguy.com\/post\/deep-learning-week3\/",
        "wordCount" : "621",
        "genre" : [ ],
        "keywords" : [ 
      
      "coursera"

    
      
        ,

      
      "deeplearning"

    
      
        ,

      
      "neuralnetworks"

    ]
    }
  </script>


</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/profile.jpg"
        alt="profile picture"
      />
      
        <div class="sidebar__introduction-title">
          <a href="/">The Tech Art Guy</a>
        </div>
      
      <div class="sidebar__introduction-description">
        <p>Bridging Art and Code</p>
      </div>
    </div>
    <ul class="sidebar__list">
      
        <li class="sidebar__list-item">
          <a
            href="https://www.linkedin.com/in/swapnil-revankar"
            target="_blank"
            rel="noopener me"
            aria-label="Linkedin"
            title="Linkedin"
          >
            <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://github.com/swapnilrevankar"
            target="_blank"
            rel="noopener me"
            aria-label="GitHub"
            title="GitHub"
          >
            <i class="fab fa-github fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://twitter.com/RevankarSr"
            target="_blank"
            rel="noopener me"
            aria-label="twitter"
            title="twitter"
          >
            <i class="fab fa-twitter fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="mailto:swapnil_revankar2004@yahoo.com"
            target="_blank"
            rel="noopener me"
            aria-label="e-mail"
            title="e-mail"
          >
            <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Swapnil Revankar
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/"
              
              title=""
              >Home</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/post/"
              
              title=""
              >Posts</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/portfolio/"
              
              title=""
              >Portfolio</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/about/"
              
              title=""
              >About</a
            >
          </li>
        

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      
        <h1>Planar Data Classification With One Hidden Layer</h1>
      
      
        <ul class="post__meta">
          <li class="post__meta-item">
            <em class="fas fa-calendar-day post__meta-icon"></em>
            <span class="post__meta-text"
              >
                
                  15/9/2025
                

              
            </span>
          </li>
          <li class="post__meta-item">
            <em class="fas fa-stopwatch post__meta-icon"></em>
            <span class="post__meta-text">3-minute read</span>
          </li>
        </ul>
      <p>For Week 3 of the Deep Learning Specialization, we move beyond logistic regression and build our <strong>first neural network</strong> — one with a single hidden layer. The task is to classify a toy <strong>flower-shaped dataset</strong>, which logistic regression cannot handle well.</p>
<hr>
<h2 id="the-problem">The Problem</h2>
<p>The dataset looks like a set of <strong>flower petals</strong> spread around the origin. Logistic regression struggles here because the decision boundary is <strong>non-linear</strong>.</p>
<figure class="small"><img src="/deep_learning-week3/flower_dataset.png"
    alt="Flower-shaped planar dataset used in Week 3"><figcaption>
      <p>Flower-shaped planar dataset used in Week 3</p>
    </figcaption>
</figure>

<hr>
<h2 id="neural-network-architecture">Neural Network Architecture</h2>
<p>We design a <strong>2-layer neural network</strong>:</p>
<ul>
<li><strong>Input layer</strong>: 2 features (x₁, x₂)</li>
<li><strong>Hidden layer</strong>: 4 neurons with <code>tanh</code> activation</li>
<li><strong>Output layer</strong>: 1 neuron with <code>sigmoid</code> activation (binary classification)</li>
</ul>
<hr>
<h2 id="forward-propagation">Forward Propagation</h2>
<p>Forward propagation computes the activations layer by layer.
+
Mathematically:</p>
<p>$$
Z^{[1]} = W^{[1]} X + b^{[1]} \newline
A^{[1]} = \tanh(Z^{[1]}) \newline
Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} \newline
\hat{Y} = A^{[2]} = \sigma(Z^{[2]})
$$</p>
<p>In code, forward pass looks like:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;W1&#34;</span><span class="p">],</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;b1&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;W2&#34;</span><span class="p">],</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;b2&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
</span></span><span class="line"><span class="cl">    <span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
</span></span><span class="line"><span class="cl">    <span class="n">A2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cache</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;Z1&#34;</span><span class="p">:</span> <span class="n">Z1</span><span class="p">,</span> <span class="s2">&#34;A1&#34;</span><span class="p">:</span> <span class="n">A1</span><span class="p">,</span> <span class="s2">&#34;Z2&#34;</span><span class="p">:</span> <span class="n">Z2</span><span class="p">,</span> <span class="s2">&#34;A2&#34;</span><span class="p">:</span> <span class="n">A2</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span>
</span></span></code></pre></div><h2 id="cost-function--cross-entropy-loss">Cost Function – Cross-Entropy Loss</h2>
<p>We use the standard <strong>cross-entropy loss</strong>:</p>
<p>$$
J = -\frac{1}{m} \sum_{i=1}^{m} \Big[ y^{(i)} \log(\hat{y}^{(i)}) + (1-y^{(i)}) \log(1-\hat{y}^{(i)}) \Big]
$$</p>
<p>This penalizes confident wrong predictions heavily, encouraging the network to output probabilities close to the true labels.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># number of examples</span>
</span></span><span class="line"><span class="cl">    <span class="n">logprobs</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A2</span><span class="p">),</span><span class="n">Y</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A2</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>  <span class="c1"># ensure it&#39;s a scalar</span>
</span></span></code></pre></div><h2 id="backward-propagation">Backward Propagation</h2>
<p>The key to training is computing gradients:</p>
<p>$$
dZ^{[2]} = A^{[2]} - Y \newline
dW^{[2]} = \frac{1}{m} dZ^{[2]} A^{[1]T} \newline
db^{[2]} = \frac{1}{m} \sum dZ^{[2]}
$$</p>
<p>For the hidden layer:</p>
<p>$$
dZ^{[1]} = (W^{[2]T} dZ^{[2]}) \odot (1 - A^{[1]^2}) \newline
dW^{[1]} = \frac{1}{m} dZ^{[1]} X^T \newline
db^{[1]} = \frac{1}{m} \sum dZ^{[1]}
$$</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;W2&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">A1</span><span class="p">,</span> <span class="n">A2</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&#34;A1&#34;</span><span class="p">],</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&#34;A2&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dZ2</span> <span class="o">=</span> <span class="n">A2</span> <span class="o">-</span> <span class="n">Y</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">dW1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;dW1&#34;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span> <span class="s2">&#34;db1&#34;</span><span class="p">:</span> <span class="n">db1</span><span class="p">,</span> <span class="s2">&#34;dW2&#34;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span> <span class="s2">&#34;db2&#34;</span><span class="p">:</span> <span class="n">db2</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">grads</span>
</span></span></code></pre></div><h2 id="parameter-update">Parameter Update</h2>
<p>We update parameters using gradient descent:</p>
<p>$$
W^{[l]} := W^{[l]} - \alpha , dW^{[l]} \newline
b^{[l]} := b^{[l]} - \alpha , db^{[l]}
$$</p>
<p>where \(\alpha\) is the learning rate.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;W1&#34;</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;dW1&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;b1&#34;</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;db1&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;W2&#34;</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;dW2&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">parameters</span><span class="p">[</span><span class="s2">&#34;b2&#34;</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&#34;db2&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">parameters</span>
</span></span></code></pre></div><h2 id="putting-it-all-together">Putting It All Together</h2>
<p>Now that we have forward propagation, cost computation, backward propagation, and parameter updates, we can combine them into one training loop.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">nn_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_x</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialize parameters</span>
</span></span><span class="line"><span class="cl">    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Forward propagation</span>
</span></span><span class="line"><span class="cl">        <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Compute cost</span>
</span></span><span class="line"><span class="cl">        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Backward propagation</span>
</span></span><span class="line"><span class="cl">        <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Update parameters</span>
</span></span><span class="line"><span class="cl">        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">parameters</span>
</span></span></code></pre></div><h2 id="results">Results</h2>
<ul>
<li>Logistic regression achieves only ~47% accuracy on this dataset.</li>
<li>Our 2-layer neural network achieves <strong>~90%+ accuracy</strong>.</li>
<li>The decision boundary is non-linear and adapts to the flower shape.</li>
</ul>
<figure class="small"><img src="/deep_learning-week3/decision_boundary.png"
    alt="Decision boundary learned by the hidden-layer neural network"><figcaption>
      <p>Decision boundary learned by the hidden-layer neural network</p>
    </figcaption>
</figure>

<h2 id="key-takeaways">Key Takeaways</h2>
<ul>
<li>Adding a hidden layer lets us capture <strong>non-linear patterns</strong>.</li>
<li><code>tanh</code> works well for hidden layers, while <code>sigmoid</code> is used for binary output.</li>
<li>Forward + backward propagation form the <strong>core training loop</strong>.</li>
<li>Even a shallow network can vastly outperform logistic regression on complex data.</li>
</ul>



<h3>Posts in this series</h3>
<ul>
  
    <li><a href="/post/deep-learning-week3/">Planar Data Classification With One Hidden Layer</a></li>
  
    <li><a href="/post/deep-learning-week1-and-week2/">From Math to Code: Logistic Regression in Neural Networks</a></li>
  
</ul>
</div>
    <div class="post__footer">
      

      
        <span><a class="tag" href="/tags/coursera/">coursera</a><a class="tag" href="/tags/deeplearning/">deeplearning</a><a class="tag" href="/tags/neuralnetworks/">neuralnetworks</a></span>


      
    </div>

    
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Swapnil Revankar
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous" /><script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH" crossorigin="anonymous"></script><script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
      integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script></body>
</html>
